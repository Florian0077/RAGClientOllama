# RAG Application avec Ollama et Streamlit

Application de Retrieval-Augmented Generation (RAG) utilisant Ollama et Streamlit pour crÃ©er un chatbot intelligent capable de rÃ©pondre Ã  des questions basÃ©es sur vos documents.

## FonctionnalitÃ©s

- ğŸ’¬ Interface de chat intuitive
- ğŸ“š Indexation de documents (PDF, TXT, MD, CSV)
- ğŸ” Recherche sÃ©mantique dans les documents
- ğŸ¤– Utilisation du modÃ¨le llama3.2-vision
- ğŸ“Š Affichage des sources et extraits pertinents
- ğŸ—‘ï¸ Gestion des documents (ajout/suppression)

## PrÃ©requis

- Python 3.8+
- Ollama avec le modÃ¨le llama3.2-vision
- Un serveur Ollama accessible

## Installation

1. Cloner le repository :
```bash
git clone [URL_DU_REPO]
cd OllamaClient
```

2. CrÃ©er un environnement virtuel :
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
.\venv\Scripts\activate  # Windows
```

3. Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

## Configuration

1. Assurez-vous que le serveur Ollama est en cours d'exÃ©cution
2. VÃ©rifiez que le modÃ¨le llama3.2-vision est installÃ© :
```bash
ollama pull llama3.2-vision
```

## Utilisation

1. Lancer l'application :
```bash
streamlit run app.py
```

2. AccÃ©der Ã  l'interface web :
- Ouvrez votre navigateur Ã  l'adresse : http://localhost:8501

3. Utilisation :
- Onglet "Chat" : Posez vos questions sur vos documents
- Onglet "Indexation" : GÃ©rez vos documents (ajout/suppression)

## Structure du Projet

```
OllamaClient/
â”œâ”€â”€ app.py              # Application principale
â”œâ”€â”€ requirements.txt    # DÃ©pendances Python
â”œâ”€â”€ .gitignore         # Fichiers ignorÃ©s par Git
â”œâ”€â”€ README.md          # Documentation
â””â”€â”€ chroma_db/         # Base de donnÃ©es vectorielle (gÃ©nÃ©rÃ©)
```

## Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commit vos changements
4. Push sur la branche
5. Ouvrir une Pull Request
